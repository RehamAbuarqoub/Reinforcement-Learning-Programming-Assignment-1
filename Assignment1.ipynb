{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdcd7675",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Programming - CSCN 8020\n",
    "### Assignment 1\n",
    "* Student Name: Reham Abuarqoub \n",
    "* Student ID: 9062922\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7b1b4e",
   "metadata": {},
   "source": [
    "# Problem 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afea21f",
   "metadata": {},
   "source": [
    "## Pick-and-Place Robot as an MDP\n",
    "\n",
    "We want to design a reinforcement learning setup for a robot arm that repeats a **pick-and-place task**.  \n",
    "The robot must learn to move **fast** and **smoothly**, while controlling its motors directly and using feedback from positions and velocities.\n",
    "\n",
    "---\n",
    "\n",
    "## MDP Design\n",
    "\n",
    "### 1. States\n",
    "The state should describe both the robot and the task:\n",
    "- **Robot kinematics**: joint positions (`q`) and joint velocities (`qdot`)  \n",
    "- **Task context**: object pose (`x_obj`), goal pose (`x_goal`)  \n",
    "- **Gripper status**: open/closed (`g`), and whether the object is held (`c`)  \n",
    "- **Optional**: collision or distance-to-obstacle indicators  \n",
    "\n",
    "**Example state vector:**  \n",
    "`s = [q, qdot, x_obj, x_goal, g, c]`\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Actions\n",
    "The agent must control the motors directly:\n",
    "- **Joint torque commands** (`tau ∈ R^n`, bounded by torque limits)  \n",
    "- Alternative: joint velocity commands (if low-level controller is used)  \n",
    "\n",
    "Torque control best matches the requirement and gives smoother motion.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Transition Dynamics\n",
    "- Governed by robot physics (rigid-body dynamics with contacts).  \n",
    "- Next state depends on the applied torque, current joint positions/velocities, and possible noise.  \n",
    "- Gripper contact flag (`c`) switches on when the gripper closes around the object within tolerance.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Rewards\n",
    "We want to encourage **success**, **speed**, **smoothness**, and **safety**:\n",
    "\n",
    "- **+ Progress reward** → when the arm gets closer to the object (before grasp) or moves the object closer to the goal (after grasp).  \n",
    "- **+ Success bonus** → large positive reward when the object is placed at the goal.  \n",
    "- **− Time penalty** → small negative each step to encourage faster completion.  \n",
    "- **− Energy penalty** → penalize large torques (`‖tau‖²`).  \n",
    "- **− Smoothness penalty** → penalize jerky changes in velocity/torque.  \n",
    "- **− Collision penalty** → strong negative if the robot collides or drops the object.  \n",
    "\n",
    "\n",
    "\n",
    "### 5. Discount Factor (γ)\n",
    "- Between **0.95 – 0.995**  \n",
    "- Encourages both short-term speed and long-term smoothness.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Episode Termination\n",
    "- **Success:** object placed at the goal and gripper released.  \n",
    "- **Failure:** collision, dropped object, or time limit reached.  \n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "- **States**: include positions, velocities, object/goal info, and gripper state.  \n",
    "- **Actions**: direct motor torques for fine and smooth control.  \n",
    "- **Rewards**: balance success, speed, smoothness, and safety.  \n",
    "- This MDP formulation allows reinforcement learning to train the robot arm for efficient and natural pick-and-place movements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b2e912",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6669727b",
   "metadata": {},
   "source": [
    "## Two Iterations of Value Iteration\n",
    "\n",
    "**Setup**  \n",
    "- Grid layout:  \n",
    "  `s1  s2`  \n",
    "  `s3  s4`\n",
    "- Rewards: `R(s1)=5, R(s2)=10, R(s3)=1, R(s4)=2`\n",
    "- Discount: γ = 0.9  \n",
    "- Transition: deterministic; if action hits wall → stay in same state.  \n",
    "- Bellman backup:  \n",
    "  \\[\n",
    "  V_{k+1}(s) = \\max_a \\big[ R(s) + \\gamma V_k(s') \\big]\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### Iteration 1\n",
    "\n",
    "1) **Initial value function \\(V_0\\):**\n",
    "\n",
    "| State | V₀ |\n",
    "|-------|----|\n",
    "| s1    | 0  |\n",
    "| s2    | 0  |\n",
    "| s3    | 0  |\n",
    "| s4    | 0  |\n",
    "\n",
    "2) **Update step (future term is zero since V₀=0):**\n",
    "\n",
    "| State | Calculation          | V₁ |\n",
    "|-------|----------------------|----|\n",
    "| s1    | 5 + 0 = 5            | 5  |\n",
    "| s2    | 10 + 0 = 10          | 10 |\n",
    "| s3    | 1 + 0 = 1            | 1  |\n",
    "| s4    | 2 + 0 = 2            | 2  |\n",
    "\n",
    "3) **Updated value function \\(V_1\\):**\n",
    "\n",
    "| State | V₁ | Greedy action(s) |\n",
    "|-------|----|------------------|\n",
    "| s1    | 5  | any (all tie)    |\n",
    "| s2    | 10 | any (all tie)    |\n",
    "| s3    | 1  | any (all tie)    |\n",
    "| s4    | 2  | any (all tie)    |\n",
    "\n",
    "---\n",
    "\n",
    "### Iteration 2\n",
    "\n",
    "**Start with V₁ = {s1=5, s2=10, s3=1, s4=2}.**\n",
    "\n",
    "| State | Actions considered | Calculation (max)                  | V₂  | Greedy action |\n",
    "|-------|--------------------|-------------------------------------|-----|----------------|\n",
    "| s1    | right→s2, down→s3, stay | max(5+0.9·10, 5+0.9·1, 5+0.9·5) = max(14, 5.9, 9.5) | 14  | right |\n",
    "| s2    | left→s1, down→s4, stay | max(10+0.9·5, 10+0.9·2, 10+0.9·10) = max(14.5, 11.8, 19) | 19  | stay (up/right) |\n",
    "| s3    | up→s1, right→s4, stay | max(1+0.9·5, 1+0.9·2, 1+0.9·1) = max(5.5, 2.8, 1.9) | 5.5 | up |\n",
    "| s4    | up→s2, left→s3, stay  | max(2+0.9·10, 2+0.9·1, 2+0.9·2) = max(11, 2.9, 3.8) | 11  | up |\n",
    "\n",
    "**Updated value function \\(V_2\\):**\n",
    "\n",
    "| State | V₂  | Greedy action |\n",
    "|-------|-----|---------------|\n",
    "| s1    | 14  | right         |\n",
    "| s2    | 19  | stay (up/right) |\n",
    "| s3    | 5.5 | up            |\n",
    "| s4    | 11  | up            |\n",
    "\n",
    "---\n",
    "\n",
    "### Takeaway\n",
    "After two iterations, the value function already pushes the agent toward **s2** (the highest reward). The greedy policy shows clear direction:  \n",
    "- From s1 → move **right** to s2  \n",
    "- From s3 → move **up** to s1  \n",
    "- From s4 → move **up** to s2  \n",
    "- From s2 → best to **stay** (because it’s already the maximum reward spot).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c9daa",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf1f7be",
   "metadata": {},
   "source": [
    "## 5×5 Gridworld (Value Iteration)\n",
    "\n",
    "**Environment**\n",
    "- Grid size: 5×5. State = (row, col) with 0-based indexing.\n",
    "- **Goal:** (4,4) — terminal/absorbing, reward **+10** on arrival.\n",
    "- **Grey states:** {(0,4), (1,2), (3,0)} — reward **−5** on arrival.\n",
    "- **All other states:** reward **−1** on arrival.\n",
    "- **Actions:** Right, Left, Down, Up. If the move leaves the grid → stay in place.\n",
    "- **Discount:** γ = 0.9.\n",
    "\n",
    "**Algorithm (copy-based value iteration)**\n",
    "- Keep two arrays: `V` (current values) and `V_new` (updates).\n",
    "- For each state, compute the best one-step lookahead return using **only** `V`, write to `V_new`.\n",
    "- After the sweep, set `V = V_new`.\n",
    "- Stop when the largest change across states is below a small threshold `THETA`.\n",
    "\n",
    "The script prints the optimal value function \\(V^\\*\\) and the greedy policy \\(\\pi^\\*\\) as an arrow grid:\n",
    "- ► ◄ ▼ ▲ for actions,\n",
    "- **X** for grey cells,\n",
    "- **G** for the goal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7431a475",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1 — Reward as a List (Table)\n",
    "We’ve already built `R[r,c]` as a single source-of-truth for rewards by state:\n",
    "- Regular states = `−1`\n",
    "- Grey states = `−5`\n",
    "- Goal = `+10`\n",
    "\n",
    "This satisfies “**reward function as a list**.”\n",
    "\n",
    "### Step 2 — Copy-Based Value Iteration and Greedy Policy\n",
    "We now implement **copy-based** value iteration:\n",
    "- Build `V_new` from `V` without reusing updates within the sweep.\n",
    "- After convergence, extract the greedy policy \\( \\pi^* \\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591c99b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function found in 9 sweeps:\n",
      "[[-0.434062  0.62882   1.8098    3.122     4.58    ]\n",
      " [ 0.62882   1.8098    3.122     4.58      6.2     ]\n",
      " [ 1.8098    3.122     4.58      6.2       8.      ]\n",
      " [ 3.122     4.58      6.2       8.       10.      ]\n",
      " [ 4.58      6.2       8.       10.        0.      ]]\n",
      "['►', '►', '►', '▼', 'X']\n",
      "['►', '▼', 'X', '►', '▼']\n",
      "['►', '►', '►', '►', '▼']\n",
      "['X', '►', '►', '►', '▼']\n",
      "['►', '►', '►', '►', 'G']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Problem 3 — 5x5 Gridworld (stand-alone)\n",
    "\n",
    "Environment:\n",
    "- Deterministic transitions; invalid moves keep you in place.\n",
    "- Reward paid on ARRIVAL to the next state s':\n",
    "    +10 at the goal (4,4)  [terminal/absorbing]\n",
    "    -5  at grey states {(0,4), (1,2), (3,0)}\n",
    "    -1  otherwise\n",
    "- Discount gamma = 0.9\n",
    "\n",
    "Algorithm:\n",
    "- Copy-based value iteration (write updates into V_new, then replace V ← V_new).\n",
    "- Prints optimal V* and the greedy policy (arrows), with X for grey and G for goal.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "N = 5\n",
    "GAMMA = 0.9\n",
    "THETA = 1e-6\n",
    "MAX_ITERS = 10_000\n",
    "\n",
    "GOAL  = (4, 4)\n",
    "GREYS = {(0, 4), (1, 2), (3, 0)}  # match the figure in the prompt\n",
    "\n",
    "# action index -> (Δrow, Δcol)\n",
    "ACTIONS = {\n",
    "    0: (0, 1),    # Right\n",
    "    1: (0, -1),   # Left\n",
    "    2: (1, 0),    # Down\n",
    "    3: (-1, 0),   # Up\n",
    "}\n",
    "ARROW = {0: \"►\", 1: \"◄\", 2: \"▼\", 3: \"▲\"}\n",
    "\n",
    "# -----------------------------\n",
    "# Environment helpers\n",
    "# -----------------------------\n",
    "def in_bounds(r: int, c: int) -> bool:\n",
    "    \"\"\"True if (r, c) is inside the 5x5 grid.\"\"\"\n",
    "    return 0 <= r < N and 0 <= c < N\n",
    "\n",
    "def step(state: tuple[int, int], a_idx: int):\n",
    "    \"\"\"\n",
    "    Deterministic transition; wall -> stay.\n",
    "    Returns: (next_state, reward_on_arrival, done)\n",
    "    \"\"\"\n",
    "    if state == GOAL:\n",
    "        return state, 0.0, True  # absorbing goal\n",
    "\n",
    "    dr, dc = ACTIONS[a_idx]\n",
    "    r, c   = state\n",
    "    nr, nc = r + dr, c + dc\n",
    "    if not in_bounds(nr, nc):\n",
    "        nr, nc = r, c  # hit wall: stay\n",
    "\n",
    "    s_next = (nr, nc)\n",
    "    if s_next == GOAL:\n",
    "        return s_next, 10.0, True\n",
    "    if s_next in GREYS:\n",
    "        return s_next, -5.0, False\n",
    "    return s_next, -1.0, False\n",
    "\n",
    "# -----------------------------\n",
    "# Value Iteration (copy-based)\n",
    "# -----------------------------\n",
    "def value_iteration():\n",
    "    \"\"\"\n",
    "    Copy-based value iteration:\n",
    "      - Build V_new from the current V (no reuse within the sweep),\n",
    "      - Replace V ← V_new after each sweep,\n",
    "      - Stop when the max change is below THETA.\n",
    "    Returns: (V*, number_of_sweeps)\n",
    "    \"\"\"\n",
    "    V = np.zeros((N, N), dtype=float)  # V(GOAL)=0; the +10 is paid on arrival\n",
    "    for it in range(MAX_ITERS):\n",
    "        delta = 0.0\n",
    "        V_new = V.copy()\n",
    "        for r in range(N):\n",
    "            for c in range(N):\n",
    "                s = (r, c)\n",
    "                if s == GOAL:\n",
    "                    V_new[r, c] = 0.0\n",
    "                    continue\n",
    "\n",
    "                # One-step lookahead: max over actions of r + gamma * V(next)\n",
    "                best_q = -1e18\n",
    "                for a_idx in ACTIONS:\n",
    "                    s_next, rwd, done = step(s, a_idx)\n",
    "                    nr, nc = s_next\n",
    "                    q = rwd + (0.0 if done else GAMMA * V[nr, nc])\n",
    "                    if q > best_q:\n",
    "                        best_q = q\n",
    "\n",
    "                delta = max(delta, abs(best_q - V[r, c]))\n",
    "                V_new[r, c] = best_q\n",
    "\n",
    "        V = V_new\n",
    "        if delta < THETA:\n",
    "            return V, it + 1\n",
    "\n",
    "    return V, MAX_ITERS\n",
    "\n",
    "def greedy_policy_from_V(V: np.ndarray):\n",
    "    \"\"\"\n",
    "    Build a printable grid of the greedy policy:\n",
    "      ► ◄ ▼ ▲ for regular cells, X for greys, G for goal.\n",
    "    \"\"\"\n",
    "    Pi = np.empty((N, N), dtype=object)\n",
    "    for r in range(N):\n",
    "        for c in range(N):\n",
    "            s = (r, c)\n",
    "            if s == GOAL:\n",
    "                Pi[r, c] = \"G\"\n",
    "                continue\n",
    "            if s in GREYS:\n",
    "                Pi[r, c] = \"X\"\n",
    "                continue\n",
    "\n",
    "            best_a, best_q = None, -1e18\n",
    "            for a_idx in ACTIONS:\n",
    "                s_next, rwd, done = step(s, a_idx)\n",
    "                nr, nc = s_next\n",
    "                q = rwd + (0.0 if done else GAMMA * V[nr, nc])\n",
    "                if q > best_q:\n",
    "                    best_q, best_a = q, a_idx\n",
    "\n",
    "            Pi[r, c] = ARROW[best_a]\n",
    "    return Pi\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    V_star, iters = value_iteration()\n",
    "    np.set_printoptions(precision=6, suppress=True)\n",
    "    print(f\"Optimal Value Function found in {iters} sweeps:\")\n",
    "    print(V_star)\n",
    "\n",
    "    pi_star = greedy_policy_from_V(V_star)\n",
    "    for row in pi_star:\n",
    "        print(list(row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59baa84",
   "metadata": {},
   "source": [
    "\n",
    "- A convergence message like: `Converged in 9 sweeps.` (exact sweeps can vary slightly with tolerance).\n",
    "- A nicely formatted **value table** where `G` marks the goal and `X` marks grey states.\n",
    "- A matching **arrow policy** that moves toward `(4,4)` while avoiding/going around greys when optimal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56752f0d",
   "metadata": {},
   "source": [
    "##  Task 2: In-Place Value Iteration (with comparison)\n",
    "\n",
    "**What changes from Task 1?**  \n",
    "- We keep **one** value table `V`.  \n",
    "- When we back up a state `(r,c)`, we **write the new value directly into `V[r,c]`** and then **reuse** that fresh value for subsequent states in the same sweep.  \n",
    "- This often reduces the number of sweeps to converge because each backup can benefit from the most recent neighbors.\n",
    "\n",
    "**Goal.** Show that in-place value iteration converges to the **same** optimal value function \\(V^\\*\\) and greedy policy \\(\\pi^\\*\\) as the copy-based method from Task 1, and briefly compare performance and complexity.\n",
    "\n",
    "**Environment (same as Task 1).**  \n",
    "- Grid 5×5, goal `(4,4)` with **+10** on arrival (terminal).  \n",
    "- Grey cells `{(0,4), (1,2), (3,0)}` with **−5** on arrival.  \n",
    "- All other arrivals **−1**.  \n",
    "- Actions: Right, Left, Down, Up (deterministic; walls ⇒ stay).  \n",
    "- Discount `γ = 0.9`.\n",
    "\n",
    "**What to look at in the output.**  \n",
    "- The printed `V* (in-place)` values and the arrow policy grid (► ◄ ▼ ▲; **X** grey; **G** goal).  \n",
    "- A small summary that:  \n",
    "  1) checks the **numerical equality** of `V*` from in-place vs copy-based,  \n",
    "  2) reports **number of sweeps** and **wall-clock time** for both.  \n",
    "*(There is no “number of episodes” here—value iteration is a planning algorithm, not Monte Carlo.)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b179cacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== In-Place Value Iteration (Task 2) ===\n",
      "Converged in 9 sweeps, time=0.0031s\n",
      "V* (in-place):\n",
      " [[-0.434062  0.62882   1.8098    3.122     4.58    ]\n",
      " [ 0.62882   1.8098    3.122     4.58      6.2     ]\n",
      " [ 1.8098    3.122     4.58      6.2       8.      ]\n",
      " [ 3.122     4.58      6.2       8.       10.      ]\n",
      " [ 4.58      6.2       8.       10.        0.      ]]\n",
      "π* (► ◄ ▼ ▲; X=grey; G=goal):\n",
      "['►', '►', '►', '▼', 'X']\n",
      "['►', '▼', 'X', '►', '▼']\n",
      "['►', '►', '►', '►', '▼']\n",
      "['X', '►', '►', '►', '▼']\n",
      "['►', '►', '►', '►', 'G']\n",
      "\n",
      "=== Comparison with copy-based VI (Task 1) ===\n",
      "Copy-based:  sweeps=9, time=0.0022s\n",
      "In-place  :  sweeps=9, time=0.0031s\n",
      "Do both methods reach the same V*?  YES\n"
     ]
    }
   ],
   "source": [
    "# Task 2: In-Place Value Iteration (stand-alone, includes a comparison vs copy-based)\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "N         = 5\n",
    "GAMMA     = 0.9\n",
    "THETA     = 1e-6\n",
    "MAX_ITERS = 10_000\n",
    "\n",
    "GOAL  = (4, 4)\n",
    "GREYS = {(0, 4), (1, 2), (3, 0)}  # as in the figure\n",
    "\n",
    "ACTIONS = {\n",
    "    0: (0, 1),   # Right\n",
    "    1: (0, -1),  # Left\n",
    "    2: (1, 0),   # Down\n",
    "    3: (-1, 0),  # Up\n",
    "}\n",
    "ARROW = {0: \"►\", 1: \"◄\", 2: \"▼\", 3: \"▲\"}\n",
    "\n",
    "# -----------------------------\n",
    "# Environment helpers\n",
    "# -----------------------------\n",
    "def in_bounds(r, c):\n",
    "    return 0 <= r < N and 0 <= c < N\n",
    "\n",
    "def step(state, a_idx):\n",
    "    \"\"\"Deterministic transition; walls keep you in place. Reward paid on ARRIVAL.\"\"\"\n",
    "    if state == GOAL:\n",
    "        return state, 0.0, True\n",
    "    dr, dc = ACTIONS[a_idx]\n",
    "    r, c   = state\n",
    "    nr, nc = r + dr, c + dc\n",
    "    if not in_bounds(nr, nc):   # wall -> stay put\n",
    "        nr, nc = r, c\n",
    "    s_next = (nr, nc)\n",
    "    if s_next == GOAL:  return s_next, 10.0, True\n",
    "    if s_next in GREYS: return s_next, -5.0, False\n",
    "    return s_next, -1.0, False\n",
    "\n",
    "def policy_grid_from_V(V):\n",
    "    \"\"\"Pretty grid: ► ◄ ▼ ▲; X for grey; G for goal (greedy one-step lookahead).\"\"\"\n",
    "    grid = []\n",
    "    for r in range(N):\n",
    "        row = []\n",
    "        for c in range(N):\n",
    "            s = (r, c)\n",
    "            if s == GOAL:  row.append(\"G\"); continue\n",
    "            if s in GREYS: row.append(\"X\"); continue\n",
    "            best_a, best_q = None, -1e18\n",
    "            for a_idx in ACTIONS:\n",
    "                (nr, nc), rwd, done = *step(s, a_idx),\n",
    "                q = rwd + (0.0 if done else GAMMA * V[nr, nc])\n",
    "                if q > best_q:\n",
    "                    best_q, best_a = q, a_idx\n",
    "            row.append(ARROW[best_a])\n",
    "        grid.append(row)\n",
    "    return grid\n",
    "\n",
    "# -----------------------------\n",
    "# In-Place Value Iteration (this task)\n",
    "# -----------------------------\n",
    "def value_iteration_inplace():\n",
    "    \"\"\"\n",
    "    In-place update:\n",
    "      For each state in the sweep, compute the backup using the *current* V and\n",
    "      immediately write V[r,c] = backup_value (then reuse it in the same sweep).\n",
    "    \"\"\"\n",
    "    V = np.zeros((N, N), dtype=float)\n",
    "    t0 = time.perf_counter()\n",
    "    for it in range(MAX_ITERS):\n",
    "        delta = 0.0\n",
    "        for r in range(N):\n",
    "            for c in range(N):\n",
    "                s = (r, c)\n",
    "                if s == GOAL:\n",
    "                    V[r, c] = 0.0\n",
    "                    continue\n",
    "                v_old = V[r, c]\n",
    "                best  = -1e18\n",
    "                for a_idx in ACTIONS:\n",
    "                    (nr, nc), rwd, done = *step(s, a_idx),\n",
    "                    # uses up-to-date V inside this sweep\n",
    "                    q = rwd + (0.0 if done else GAMMA * V[nr, nc])\n",
    "                    if q > best:\n",
    "                        best = q\n",
    "                V[r, c] = best\n",
    "                delta   = max(delta, abs(best - v_old))\n",
    "        if delta < THETA:\n",
    "            return V, it + 1, time.perf_counter() - t0\n",
    "    return V, MAX_ITERS, time.perf_counter() - t0\n",
    "\n",
    "# -----------------------------\n",
    "# Copy-based Value Iteration (for comparison)\n",
    "# -----------------------------\n",
    "def value_iteration_copy_based():\n",
    "    \"\"\"\n",
    "    Copy-based update:\n",
    "      Build V_new from old V (no reuse within the sweep), then V <- V_new.\n",
    "    \"\"\"\n",
    "    V = np.zeros((N, N), dtype=float)\n",
    "    t0 = time.perf_counter()\n",
    "    for it in range(MAX_ITERS):\n",
    "        delta = 0.0\n",
    "        V_new = V.copy()\n",
    "        for r in range(N):\n",
    "            for c in range(N):\n",
    "                s = (r, c)\n",
    "                if s == GOAL:\n",
    "                    V_new[r, c] = 0.0\n",
    "                    continue\n",
    "                best = -1e18\n",
    "                for a_idx in ACTIONS:\n",
    "                    (nr, nc), rwd, done = *step(s, a_idx),\n",
    "                    q = rwd + (0.0 if done else GAMMA * V[nr, nc])  # read old V\n",
    "                    if q > best:\n",
    "                        best = q\n",
    "                delta = max(delta, abs(best - V[r, c]))\n",
    "                V_new[r, c] = best\n",
    "        V = V_new\n",
    "        if delta < THETA:\n",
    "            return V, it + 1, time.perf_counter() - t0\n",
    "    return V, MAX_ITERS, time.perf_counter() - t0\n",
    "\n",
    "# -----------------------------\n",
    "# Run task + comparison\n",
    "# -----------------------------\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "V_inp, it_inp, t_inp = value_iteration_inplace()\n",
    "print(\"=== In-Place Value Iteration (Task 2) ===\")\n",
    "print(f\"Converged in {it_inp} sweeps, time={t_inp:.4f}s\")\n",
    "print(\"V* (in-place):\\n\", V_inp)\n",
    "print(\"π* (► ◄ ▼ ▲; X=grey; G=goal):\")\n",
    "for row in policy_grid_from_V(V_inp):\n",
    "    print(row)\n",
    "\n",
    "# Optional: compare with copy-based to confirm same optimum\n",
    "V_cpy, it_cpy, t_cpy = value_iteration_copy_based()\n",
    "same = np.allclose(V_inp, V_cpy, atol=1e-10)\n",
    "print(\"\\n=== Comparison with copy-based VI (Task 1) ===\")\n",
    "print(f\"Copy-based:  sweeps={it_cpy}, time={t_cpy:.4f}s\")\n",
    "print(f\"In-place  :  sweeps={it_inp}, time={t_inp:.4f}s\")\n",
    "print(\"Do both methods reach the same V*? \", \"YES\" if same else \"NO\")\n",
    "\n",
    "# Note: there is no concept of 'episodes' in value iteration (that applies to Monte Carlo).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe42e979",
   "metadata": {},
   "source": [
    "### Estimated Value Function \\(V^*\\) — 5×5 Gridworld\n",
    "\n",
    "Environment:\n",
    "- Goal (terminal): **(4,4)** with \\(R=+10\\) **on arrival**; \\(V(\\text{goal})=0\\) (absorbing).\n",
    "- Grey states: **(1,2), (3,0), (0,4)** with \\(R=-5\\) **on arrival**.\n",
    "- Other states: \\(R=-1\\) **on arrival**.\n",
    "- Actions: right, left, down, up; invalid moves keep you in place.\n",
    "- Discount: \\(gamma=0.9\\).\n",
    "\n",
    "Below is the **optimal state-value function \\(V^*\\)** for **every** state \\((r,c)\\), computed via value iteration (copy-based).  \n",
    "Values are shown to **2 decimal places**. `G` marks the terminal goal; greys are included numerically since the assignment asks for *each state*.\n",
    "\n",
    "| r\\c |    0   |   1    |   2    |   3    |   4   |\n",
    "|-----|:------:|:------:|:------:|:------:|:-----:|\n",
    "| **0** | -0.43 |  0.63  |  1.81  |  3.12  |  4.58 |\n",
    "| **1** |  0.63 |  1.81  |  3.12  |  4.58  |  6.20 |\n",
    "| **2** |  1.81 |  3.12  |  4.58  |  6.20  |  8.00 |\n",
    "| **3** |  3.12 |  4.58  |  6.20  |  8.00  | 10.00 |\n",
    "| **4** |  4.58 |  6.20  |  8.00  | 10.00  |   G   |\n",
    "\n",
    "**Notes**\n",
    "- The goal state’s value is **0** because it is terminal; the **+10** is received upon **arrival** to it.\n",
    "- Grey states still have **positive** optimal values due to proximity to the goal and discounting, even though arriving **into** them yields \\(-5\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "852c9956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V* (full precision array):\n",
      " [[-0.434062  0.62882   1.8098    3.122     4.58    ]\n",
      " [ 0.62882   1.8098    3.122     4.58      6.2     ]\n",
      " [ 1.8098    3.122     4.58      6.2       8.      ]\n",
      " [ 3.122     4.58      6.2       8.       10.      ]\n",
      " [ 4.58      6.2       8.       10.        0.      ]]\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Reproduce the exact V* numerically (copy-based value iteration)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "N = 5\n",
    "GAMMA = 0.9\n",
    "GOAL  = (4, 4)\n",
    "GREYS = {(1, 2), (3, 0), (0, 4)}           # grey cells\n",
    "\n",
    "ACTIONS = {0:(0,1), 1:(0,-1), 2:(1,0), 3:(-1,0)}  # right, left, down, up\n",
    "\n",
    "def in_bounds(r, c): return 0 <= r < N and 0 <= c < N\n",
    "\n",
    "# Reward table: -1 regular, -5 grey, +10 goal\n",
    "R = np.full((N, N), -1.0)\n",
    "for (r, c) in GREYS:\n",
    "    R[r, c] = -5.0\n",
    "R[GOAL] = +10.0\n",
    "\n",
    "def step(state, a_idx):\n",
    "    if state == GOAL:\n",
    "        return state, 0.0, True\n",
    "    dr, dc = ACTIONS[a_idx]\n",
    "    r, c   = state\n",
    "    nr, nc = r + dr, c + dc\n",
    "    if not in_bounds(nr, nc):\n",
    "        nr, nc = r, c\n",
    "    s_next = (nr, nc)\n",
    "    reward = R[nr, nc]              # reward on arrival\n",
    "    done   = (s_next == GOAL)\n",
    "    return s_next, reward, done\n",
    "\n",
    "def value_iteration_copy(theta=1e-12, max_iters=100_000):\n",
    "    V = np.zeros((N, N), dtype=float)\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        V_new = V.copy()\n",
    "        for r in range(N):\n",
    "            for c in range(N):\n",
    "                s = (r, c)\n",
    "                if s == GOAL:\n",
    "                    V_new[r, c] = 0.0\n",
    "                    continue\n",
    "                best_q = -1e18\n",
    "                for a_idx in ACTIONS:\n",
    "                    (nr, nc), rwd, done = step(s, a_idx)\n",
    "                    q = rwd + (0.0 if done else GAMMA * V[nr, nc])\n",
    "                    if q > best_q:\n",
    "                        best_q = q\n",
    "                delta = max(delta, abs(best_q - V[r, c]))\n",
    "                V_new[r, c] = best_q\n",
    "        V = V_new\n",
    "        if delta < theta:\n",
    "            return V\n",
    "    return V\n",
    "\n",
    "V_star = value_iteration_copy()\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "print(\"V* (full precision array):\\n\", V_star)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c080d901",
   "metadata": {},
   "source": [
    "## Performance Comparison — Copy-based vs In-Place Value Iteration (human-readable)\n",
    "\n",
    "| Aspect | **Copy-based Value Iteration** | **In-Place Value Iteration** |\n",
    "|---|---|---|\n",
    "| **What it does** | Computes a full new table `V_new` from the old `V` each sweep, then replaces `V ← V_new`. No reuse within the same sweep. | Updates `V` **immediately** cell-by-cell; later cells in the same sweep can use the freshest values (Gauss–Seidel style). |\n",
    "| **Optimization time (wall-clock)** | Solid baseline. A bit of overhead from allocating/copying `V_new` each sweep; on small grids this is negligible, on large ones it adds up. | Often equal or **faster** because it can converge in fewer sweeps and avoids copying. Actual win depends on update order and cache friendliness. |\n",
    "| **Sweeps to converge (typical behavior)** | Predictable but may take **more** sweeps because improvements don’t propagate until the next sweep. | Often **the same or fewer** sweeps since improvements ripple forward within the sweep. A good update order (e.g., sweeping from goal outward) helps. |\n",
    "| **Number of episodes** | **N/A (0)** — This is Dynamic Programming using the model; it does not simulate trajectories. | **N/A (0)** — Same reason; no rollouts/episodes are generated. |\n",
    "| **Time complexity** | \\(O(\\text{sweeps}\\cdot|S|\\cdot|A|)\\). Each backup is \\(O(|A|)\\); total is backups × sweeps. | \\(O(\\text{sweeps}\\cdot|S|\\cdot|A|)\\). Same per-backup cost; usually fewer sweeps in practice if the ordering is favorable. |\n",
    "| **Memory footprint** | \\(O(|S|)\\) for `V` **plus** \\(O(|S|)\\) for `V_new` → effectively ~2× the value memory. | \\(O(|S|)\\) — only one value table in memory. Helpful on large state spaces. |\n",
    "| **Parallelization / Vectorization** | **Easier** to parallelize a sweep (no data hazards because `V_new` reads from `V`). Plays nicely with GPUs/NumPy vectorization. | **Harder** to parallelize (write-after-read hazards). Often runs best as a single-threaded, cache-friendly loop. |\n",
    "| **Sensitivity to update order** | **Low** — order inside a sweep doesn’t change the mathematics (only floating-point minutiae). | **High** — a smart ordering (e.g., from states near the goal outward) can noticeably reduce sweeps. A poor ordering can erase the advantage. |\n",
    "| **Stability / Debuggability** | Very **transparent**: you can log `V` and `V_new` to see exactly what changed per sweep. | Slightly trickier to debug because values change in place; logging needs care to avoid mixing old/new states. |\n",
    "| **Where it shines** | When you want clarity, easy theory, or to leverage parallel/vectorized hardware; when memory is not tight. | When memory is tight, or you want faster propagation with a good update order; often great on CPUs for large tabular MDPs. |\n",
    "| **Gotchas / Pitfalls** | Extra memory/copy cost may matter on very large `|S|`. | Order dependence; harder to parallelize; if you pick a poor order, you might not see sweep reductions. |\n",
    "| **Concrete example (this 5×5, γ=0.9)** | Converges in ~**9 sweeps**; wall-clock is tiny on a laptop (milliseconds). | Also ~**9 sweeps** with row-major order; could be **fewer** with a goal-centric order. Values and greedy policy **match** copy-based. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a78125",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5829e2f",
   "metadata": {},
   "source": [
    "## Off-Policy Monte Carlo with Importance Sampling\n",
    "\n",
    "**Setting (same gridworld as Problem 3)**  \n",
    "- **Goal:** (4,4) → reward **+10** on arrival (terminal/absorbing)  \n",
    "- **Grey cells:** {(0,4), (1,2), (3,0)} → reward **−5** on arrival  \n",
    "- **Other cells:** reward **−1** on arrival  \n",
    "- **Actions:** Right, Left, Down, Up (deterministic; walls ⇒ stay)  \n",
    "- **Discount:** \\(\\gamma = 0.9\\)\n",
    "\n",
    "**Objective**  \n",
    "Estimate the value function using **off-policy Monte Carlo (MC)** with **Weighted Importance Sampling (WIS)**:\n",
    "- **Behavior policy** \\(b\\): uniform random over actions (used to generate episodes).\n",
    "- **Target policy** \\(\\pi\\): greedy w.r.t. current \\(Q(s,a)\\) (deterministic control).\n",
    "- **Update (backward through each episode):**\n",
    "  - Return \\(G \\leftarrow \\gamma G + r\\).\n",
    "  - Cumulative weight \\(W\\) starts at 1; at each step multiply by \\(\\pi(a|s)/b(a|s)=4\\) **only** if action = current greedy; otherwise stop (ratio 0).\n",
    "  - Weighted-IS control update:\n",
    "    \\[\n",
    "      C(s,a) \\leftarrow C(s,a) + W,\\qquad\n",
    "      Q(s,a) \\leftarrow Q(s,a) + \\frac{W}{C(s,a)}\\big(G - Q(s,a)\\big)\n",
    "    \\]\n",
    "- Report \\(V_{\\pi}(s)=\\max_a Q(s,a)\\) and the greedy policy grid.\n",
    "\n",
    "**Comparison to Value Iteration (Problem 3)**  \n",
    "Also compute \\(V^*\\) with copy-based Value Iteration for reference and report timing / episodes / a simple MAE between \\(V_{\\text{MC}}\\) and \\(V^*\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f494d53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Value Iteration] sweeps=9, time=2.44 ms\n",
      "V* (Reference via Value Iteration)\n",
      " -0.43   0.63   1.81   3.12   4.58\n",
      "  0.63   1.81   3.12   4.58   6.20\n",
      "  1.81   3.12   4.58   6.20   8.00\n",
      "  3.12   4.58   6.20   8.00  10.00\n",
      "  4.58   6.20   8.00  10.00   G  \n",
      "π* (Reference via Value Iteration)\n",
      " ►  ►  ►  ▼  X\n",
      " ►  ▼  X  ►  ▼\n",
      " ►  ►  ►  ►  ▼\n",
      " X  ►  ►  ►  ▼\n",
      " ►  ►  ►  ►  G\n",
      "\n",
      "[Off-policy MC (Weighted IS)] episodes=20000, time=48374.84 ms\n",
      "V (Estimated via Off-policy MC IS)\n",
      " -0.43   0.62   1.73   3.08   4.53\n",
      "  0.62   1.80   3.11   4.55   6.17\n",
      "  1.80   3.11   4.55   6.18   7.99\n",
      "  3.11   4.56   6.18   7.98  10.00\n",
      "  4.40   6.17   7.98  10.00   G  \n",
      "π (Greedy from learned Q; arrows)\n",
      " ►  ▼  ►  ▼  X\n",
      " ►  ▼  X  ▼  ▼\n",
      " ►  ►  ►  ▼  ▼\n",
      " X  ▼  ►  ▼  ▼\n",
      " ►  ►  ►  ►  G\n"
     ]
    }
   ],
   "source": [
    "# Off-policy Monte Carlo with Importance Sampling for 5x5 Gridworld\n",
    "# Behavior b: uniform over 4 actions; Target π: greedy w.r.t learned Q\n",
    "# Also compute reference V* by Value Iteration to compare\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------\n",
    "# Environment (same as Problem 3)\n",
    "# -----------------------------\n",
    "N = 5\n",
    "GAMMA = 0.9\n",
    "GOAL  = (4, 4)\n",
    "GREYS = {(1, 2), (3, 0), (0, 4)}  # per spec\n",
    "\n",
    "# Actions: 0=right, 1=left, 2=down, 3=up\n",
    "ACTIONS = {0:(0,1), 1:(0,-1), 2:(1,0), 3:(-1,0)}\n",
    "ARROW   = {0:\"►\", 1:\"◄\", 2:\"▼\", 3:\"▲\"}\n",
    "\n",
    "def in_bounds(r, c): return 0 <= r < N and 0 <= c < N\n",
    "\n",
    "# Reward table (state-based reward on arrival to s')\n",
    "R = np.full((N, N), -1.0, dtype=float)          # regular\n",
    "for (gr, gc) in GREYS: R[gr, gc] = -5.0         # greys\n",
    "R[GOAL] = +10.0                                  # goal\n",
    "\n",
    "def step(state, a_idx):\n",
    "    \"\"\"Deterministic transition; invalid -> stay. Reward on arrival.\"\"\"\n",
    "    if state == GOAL:\n",
    "        return state, 0.0, True\n",
    "    dr, dc = ACTIONS[a_idx]\n",
    "    r, c   = state\n",
    "    nr, nc = r + dr, c + dc\n",
    "    if not in_bounds(nr, nc):\n",
    "        nr, nc = r, c\n",
    "    s_next = (nr, nc)\n",
    "    reward = R[nr, nc]\n",
    "    done   = (s_next == GOAL)\n",
    "    return s_next, reward, done\n",
    "\n",
    "# Pretty printers\n",
    "def value_table_to_str(V, title=\"V table\"):\n",
    "    lines = [title]\n",
    "    for r in range(N):\n",
    "        row = []\n",
    "        for c in range(N):\n",
    "            if (r, c) == GOAL:\n",
    "                row.append(\"  G  \")\n",
    "            else:\n",
    "                row.append(f\"{V[r,c]:6.2f}\")\n",
    "        lines.append(\" \".join(row))\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def policy_to_str(Pi, title=\"Policy\"):\n",
    "    lines = [title]\n",
    "    for r in range(N):\n",
    "        lines.append(\" \".join(f\"{cell:>2}\" for cell in Pi[r]))\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def greedy_policy_from_V(V):\n",
    "    Pi = np.empty((N, N), dtype=object)\n",
    "    for r in range(N):\n",
    "        for c in range(N):\n",
    "            s = (r, c)\n",
    "            if s == GOAL:\n",
    "                Pi[r, c] = \"G\"\n",
    "                continue\n",
    "            if s in GREYS:\n",
    "                Pi[r, c] = \"X\"\n",
    "                continue\n",
    "            best_a, best_q = None, -1e18\n",
    "            for a_idx in ACTIONS:\n",
    "                (nr, nc), rwd, done = step(s, a_idx)\n",
    "                q = rwd + (0.0 if done else GAMMA * V[nr, nc])\n",
    "                if q > best_q:\n",
    "                    best_q, best_a = q, a_idx\n",
    "            Pi[r, c] = ARROW[best_a]\n",
    "    return Pi\n",
    "\n",
    "# -----------------------------\n",
    "# Value Iteration (reference)\n",
    "# -----------------------------\n",
    "def value_iteration_copy(theta=1e-9, max_iters=100_000):\n",
    "    V = np.zeros((N, N), dtype=float)\n",
    "    for it in range(max_iters):\n",
    "        delta = 0.0\n",
    "        V_new = V.copy()\n",
    "        for r in range(N):\n",
    "            for c in range(N):\n",
    "                s = (r, c)\n",
    "                if s == GOAL:\n",
    "                    V_new[r, c] = 0.0\n",
    "                    continue\n",
    "                best_q = -1e18\n",
    "                for a_idx in ACTIONS:\n",
    "                    (nr, nc), rwd, done = step(s, a_idx)\n",
    "                    q = rwd + (0.0 if done else GAMMA * V[nr, nc])\n",
    "                    if q > best_q:\n",
    "                        best_q = q\n",
    "                delta = max(delta, abs(best_q - V[r, c]))\n",
    "                V_new[r, c] = best_q\n",
    "        V = V_new\n",
    "        if delta < theta:\n",
    "            return V, it + 1\n",
    "    return V, max_iters\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "V_star_ref, sweeps_ref = value_iteration_copy()\n",
    "t1 = time.perf_counter()\n",
    "Pi_star_ref = greedy_policy_from_V(V_star_ref)\n",
    "vi_time_ms = (t1 - t0) * 1000.0\n",
    "\n",
    "print(f\"[Value Iteration] sweeps={sweeps_ref}, time={vi_time_ms:.2f} ms\")\n",
    "print(value_table_to_str(V_star_ref, \"V* (Reference via Value Iteration)\"))\n",
    "print(policy_to_str(Pi_star_ref, \"π* (Reference via Value Iteration)\"))\n",
    "\n",
    "# -----------------------------\n",
    "# Off-policy MC Control (Weighted IS)\n",
    "# -----------------------------\n",
    "def generate_episode_behavior(max_steps=200):\n",
    "    \"\"\"Generate a trajectory under behavior b (uniform random over actions).\"\"\"\n",
    "    # start from a random non-goal state\n",
    "    while True:\n",
    "        s0 = (np.random.randint(0, N), np.random.randint(0, N))\n",
    "        if s0 != GOAL:\n",
    "            break\n",
    "    s = s0\n",
    "    traj = []  # [(s,a,r), ...]\n",
    "    for _ in range(max_steps):\n",
    "        a = np.random.choice([0,1,2,3])  # behavior b uniform\n",
    "        s_next, r, done = step(s, a)\n",
    "        traj.append((s, a, r))\n",
    "        s = s_next\n",
    "        if done:\n",
    "            break\n",
    "    return traj\n",
    "\n",
    "def offpolicy_mc_control_weighted_is(num_episodes=20000, max_steps=200):\n",
    "    Q = np.zeros((N, N, 4), dtype=float)  # action-values\n",
    "    C = np.zeros((N, N, 4), dtype=float)  # cumulative weights\n",
    "    Pi = np.zeros((N, N), dtype=int)      # greedy target indices\n",
    "\n",
    "    b_prob = 1.0 / 4.0  # behavior prob for each action\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        ep = generate_episode_behavior(max_steps=max_steps)\n",
    "        G = 0.0\n",
    "        W = 1.0\n",
    "        # process backward\n",
    "        for (s, a, r) in reversed(ep):\n",
    "            r_row, r_col = s\n",
    "            G = GAMMA * G + r\n",
    "            C[r_row, r_col, a] += W\n",
    "            Q[r_row, r_col, a] += (W / C[r_row, r_col, a]) * (G - Q[r_row, r_col, a])\n",
    "\n",
    "            # Improve π greedily\n",
    "            Pi[r_row, r_col] = np.argmax(Q[r_row, r_col, :])\n",
    "\n",
    "            # If the episode's action diverges from greedy at this (s), stop weighting earlier steps\n",
    "            if a != Pi[r_row, r_col]:\n",
    "                break\n",
    "\n",
    "            # Update importance weight (π(a|s)=1 for greedy; b(a|s)=1/4)\n",
    "            W *= (1.0 / b_prob)\n",
    "\n",
    "            # Optional guard against blow-up\n",
    "            if W > 1e12:\n",
    "                break\n",
    "    return Q, Pi\n",
    "\n",
    "t2 = time.perf_counter()\n",
    "Q_mc, Pi_mc_idx = offpolicy_mc_control_weighted_is(num_episodes=20000, max_steps=200)\n",
    "t3 = time.perf_counter()\n",
    "mc_time_ms = (t3 - t2) * 1000.0\n",
    "\n",
    "# Derived V and arrows from MC\n",
    "V_mc = np.max(Q_mc, axis=2)\n",
    "Pi_mc_arrow = np.empty((N, N), dtype=object)\n",
    "for r in range(N):\n",
    "    for c in range(N):\n",
    "        if (r, c) == GOAL:\n",
    "            Pi_mc_arrow[r, c] = \"G\"\n",
    "        elif (r, c) in GREYS:\n",
    "            Pi_mc_arrow[r, c] = \"X\"\n",
    "        else:\n",
    "            Pi_mc_arrow[r, c] = ARROW[Pi_mc_idx[r, c]]\n",
    "\n",
    "print(\"\\n[Off-policy MC (Weighted IS)] episodes=20000, time={:.2f} ms\".format(mc_time_ms))\n",
    "print(value_table_to_str(V_mc, \"V (Estimated via Off-policy MC IS)\"))\n",
    "print(policy_to_str(Pi_mc_arrow, \"π (Greedy from learned Q; arrows)\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d486884",
   "metadata": {},
   "source": [
    "### MC-IS vs Value Iteration — Comparison & Notes \n",
    "\n",
    "### Quick verdict\n",
    "Both implementations are correct. Your **Value Iteration (VI)** provides the reference \\(V^*\\) and \\(\\pi^*\\).  \n",
    "Your **Off-policy MC with Weighted IS** learns a value function very close to VI and a nearly-greedy policy (some arrows differ at 20k episodes — expected with IS variance). \n",
    "\n",
    "---\n",
    "\n",
    "### Side-by-side summary\n",
    "\n",
    "| Method | Driver | “Iterations” | Wall-clock | Value vs VI | Policy vs VI | Comments |\n",
    "|---|---|---:|---:|---|---|---|\n",
    "| **Value Iteration (VI)** | Model-based DP | **9 sweeps** | **2.44 ms** | Baseline \\(V^*\\) | Baseline \\(\\pi^*\\) | Deterministic backups; fastest when model is known. |\n",
    "| **MC-IS (20k episodes)** | Off-policy episodes | **20,000 eps** | **48,374.84 ms** | **Very close** (diffs ~0.02–0.18 typical) | **A few cells differ** | High-variance by nature; needs more data to exactly match \\(\\pi^*\\). |\n",
    "\n",
    "**Examples of small value diffs (VI → MC):**  \n",
    "- \\((0,2)\\): **1.81 → 1.73** (−0.08), \\((0,3)\\): **3.12 → 3.08** (−0.04), \\((4,0)\\): **4.58 → 4.40** (−0.18), \\((3,3)\\): **8.00 → 7.98** (−0.02).\n",
    "\n",
    "**Policy arrows that differ (MC vs VI):** \\((0,1), (1,3), (2,3), (3,1), (3,3)\\) …(others match). This is normal at 20k episodes.\n",
    "\n",
    "---\n",
    "\n",
    "### Complexity & behavior\n",
    "\n",
    "| Aspect | **Value Iteration (VI)** | **Off-policy MC + IS** |\n",
    "|---|---|---|\n",
    "| Needs model? | **Yes** (known transitions/rewards) | **No** (data only; uses behavior \\(b\\)) |\n",
    "| Work unit | Sweeps over all states | Episodes (trajectories) from \\(b\\) |\n",
    "| Time complexity | \\(O(\\text{sweeps}\\cdot|S|\\cdot|A|)\\) | \\(O(\\text{episodes}\\cdot \\text{avg\\_len})\\) (constant-time update per step) |\n",
    "| Memory | \\(O(|S|)\\) for \\(V\\) (+ small policy) | \\(O(|S|\\cdot|A|)\\) for \\(Q\\) and cumulative weights \\(C\\) |\n",
    "| Variance | Low (deterministic) | High (importance weights); **break** helps control variance |\n",
    "| Convergence speed | Few sweeps here (9) | Slower; improves with more episodes / better \\(b\\) |\n",
    "\n",
    "---\n",
    "\n",
    "### Why MC differs slightly (and how to tighten it)\n",
    "\n",
    "- **Finite-sample variance:** Weighted IS is noisy; with **20k** episodes you’re already close.  \n",
    "- **Greedy “break” rule:** In off-policy control, you stop the backward pass once the action differs from current greedy \\(\\pi\\). That means **early (far) states** get fewer weighted updates → slightly less accurate until episodes grow.  \n",
    "- **Behavior coverage:** Uniform \\(b\\) is fine; a better-directed \\(b\\) (still covering all actions) or **more episodes (e.g., 50k–200k)** will make the MC policy fully match VI everywhere.\n",
    "\n",
    "**If you want to show tighter agreement:**  \n",
    "- Increase to **50k+ episodes** (you already saw that your lecture-agent at 50k matches the VI arrows).  \n",
    "- Consider **per-decision importance sampling** (lower variance than trajectory-wise IS).  \n",
    "- Keep the **weight cap/early break** to prevent blow-ups.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
